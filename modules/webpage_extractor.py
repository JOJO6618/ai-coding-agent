# modules/webpage_extractor.py - ç½‘é¡µå†…å®¹æå–æ¨¡å—

import httpx
import json
from typing import Dict, Any, List, Union, Tuple
from utils.logger import setup_logger

logger = setup_logger(__name__)

async def tavily_extract(urls: Union[str, List[str]], api_key: str, extract_depth: str = "basic", max_urls: int = 1) -> Dict[str, Any]:
    """
    æ‰§è¡ŒTavilyç½‘é¡µå†…å®¹æå–
    
    Args:
        urls: è¦æå–çš„URLï¼ˆå­—ç¬¦ä¸²æˆ–åˆ—è¡¨ï¼‰
        api_key: Tavily APIå¯†é’¥
        extract_depth: æå–æ·±åº¦ (basic/advanced)
        max_urls: æœ€å¤§æå–URLæ•°é‡
    
    Returns:
        æå–ç»“æœå­—å…¸
    """
    if not api_key:
        return {"error": "Tavily APIå¯†é’¥æœªé…ç½®"}

    # ç¡®ä¿urlsæ˜¯åˆ—è¡¨
    if isinstance(urls, str):
        urls = [urls]

    # é™åˆ¶URLæ•°é‡
    urls = urls[:max_urls]

    try:
        async with httpx.AsyncClient() as client:
            response = await client.post(
                "https://api.tavily.com/extract",
                json={
                    "urls": urls,
                    "extract_depth": extract_depth,
                    "include_images": False,
                },
                headers={
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json",
                },
                timeout=60,
            )

            if response.status_code == 200:
                return response.json()
            else:
                return {"error": f"APIè¯·æ±‚å¤±è´¥: HTTP {response.status_code}"}

    except httpx.TimeoutException:
        return {"error": "è¯·æ±‚è¶…æ—¶ï¼Œç½‘é¡µå“åº”è¿‡æ…¢"}
    except httpx.RequestError as e:
        return {"error": f"ç½‘ç»œè¯·æ±‚é”™è¯¯: {str(e)}"}
    except Exception as e:
        logger.error(f"ç½‘é¡µæå–å¼‚å¸¸: {e}")
        return {"error": f"æå–å¼‚å¸¸: {str(e)}"}


def format_extract_results(results: Dict[str, Any]) -> str:
    """
    æ ¼å¼åŒ–æå–ç»“æœä¸ºç®€æ´ç‰ˆæœ¬
    
    Args:
        results: tavily_extractè¿”å›çš„ç»“æœ
    
    Returns:
        æ ¼å¼åŒ–åçš„å†…å®¹å­—ç¬¦ä¸²
    """
    if "error" in results:
        return f"âŒ æå–å¤±è´¥: {results['error']}"

    if not results.get("results"):
        return "âŒ æœªèƒ½æå–åˆ°ä»»ä½•å†…å®¹"

    formatted_parts = []
    
    # æˆåŠŸæå–çš„ç»“æœ
    for i, result in enumerate(results["results"], 1):
        url = result.get("url", "N/A")
        raw_content = result.get("raw_content", "").strip()
        
        if raw_content:
            content_length = len(raw_content)
            formatted_parts.append(f"ğŸŒ ç½‘é¡µå†…å®¹ ({content_length} å­—ç¬¦):")
            formatted_parts.append(f"ğŸ“ URL: {url}")
            formatted_parts.append("=" * 50)
            formatted_parts.append(raw_content)
            formatted_parts.append("=" * 50)
        else:
            formatted_parts.append(f"âš ï¸ URL {url} æå–åˆ°ç©ºå†…å®¹")

    # å¤±è´¥çš„URLï¼ˆå¦‚æœæœ‰ï¼‰
    if results.get("failed_results"):
        formatted_parts.append("\nâŒ æå–å¤±è´¥çš„URL:")
        for failed in results["failed_results"]:
            formatted_parts.append(f"- {failed.get('url', 'N/A')}: {failed.get('error', 'æœªçŸ¥é”™è¯¯')}")

    return "\n".join(formatted_parts)


async def extract_webpage_content(urls: Union[str, List[str]], api_key: str, extract_depth: str = "basic", max_urls: int = 1) -> Tuple[str, str]:
    """
    å®Œæ•´çš„ç½‘é¡µå†…å®¹æå–æµç¨‹
    
    Args:
        urls: è¦æå–çš„URLï¼ˆå­—ç¬¦ä¸²æˆ–åˆ—è¡¨ï¼‰
        api_key: Tavily APIå¯†é’¥
        extract_depth: æå–æ·±åº¦ (basic/advanced)
        max_urls: æœ€å¤§æå–URLæ•°é‡
    
    Returns:
        (å®Œæ•´å†…å®¹, å®Œæ•´å†…å®¹) - ä¸ºäº†å…¼å®¹æ€§è¿”å›ç›¸åŒå†…å®¹ä¸¤ä»½
    """
    # æ‰§è¡Œæå–
    results = await tavily_extract(urls, api_key, extract_depth, max_urls)
    
    # æ ¼å¼åŒ–ç»“æœ
    formatted_content = format_extract_results(results)
    
    # è¿”å›ç›¸åŒå†…å®¹ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œä¸éœ€è¦é•¿çŸ­ç‰ˆæœ¬åŒºåˆ†ï¼‰
    return formatted_content, formatted_content
# utils/conversation_manager.py - å¯¹è¯æŒä¹…åŒ–ç®¡ç†å™¨ï¼ˆé›†æˆTokenç»Ÿè®¡ï¼‰

import json
import os
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from config import DATA_DIR
import tiktoken

@dataclass
class ConversationMetadata:
    """å¯¹è¯å…ƒæ•°æ®"""
    id: str
    title: str
    created_at: str
    updated_at: str
    project_path: str
    thinking_mode: bool
    total_messages: int
    total_tools: int
    status: str = "active"  # active, archived, error

class ConversationManager:
    """å¯¹è¯æŒä¹…åŒ–ç®¡ç†å™¨"""
    
    def __init__(self):
        self.conversations_dir = Path(DATA_DIR) / "conversations"
        self.index_file = self.conversations_dir / "index.json"
        self.current_conversation_id: Optional[str] = None
        self._ensure_directories()
        self._load_index()
        
        # åˆå§‹åŒ–tiktokenç¼–ç å™¨
        try:
            self.encoding = tiktoken.get_encoding("cl100k_base")
        except Exception as e:
            print(f"âš ï¸ tiktokenåˆå§‹åŒ–å¤±è´¥: {e}")
            self.encoding = None
    
    def _ensure_directories(self):
        """ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨"""
        self.conversations_dir.mkdir(parents=True, exist_ok=True)
        
        # å¦‚æœç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºç©ºç´¢å¼•
        if not self.index_file.exists():
            self._save_index({})
    
    def _load_index(self) -> Dict:
        """åŠ è½½å¯¹è¯ç´¢å¼•"""
        try:
            if self.index_file.exists():
                with open(self.index_file, 'r', encoding='utf-8') as f:
                    content = f.read().strip()
                    if content:
                        return json.loads(content)
            return {}
        except (json.JSONDecodeError, Exception) as e:
            print(f"âš ï¸ åŠ è½½å¯¹è¯ç´¢å¼•å¤±è´¥ï¼Œå°†é‡æ–°åˆ›å»º: {e}")
            return {}
    
    def _save_index(self, index: Dict):
        """ä¿å­˜å¯¹è¯ç´¢å¼•"""
        try:
            with open(self.index_file, 'w', encoding='utf-8') as f:
                json.dump(index, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âŒ˜ ä¿å­˜å¯¹è¯ç´¢å¼•å¤±è´¥: {e}")
    
    def _generate_conversation_id(self) -> str:
        """ç”Ÿæˆå”¯ä¸€çš„å¯¹è¯ID"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        # æ·»åŠ æ¯«ç§’ç¡®ä¿å”¯ä¸€æ€§
        ms = int(time.time() * 1000) % 1000
        return f"conv_{timestamp}_{ms:03d}"
    
    def _get_conversation_file_path(self, conversation_id: str) -> Path:
        """è·å–å¯¹è¯æ–‡ä»¶è·¯å¾„"""
        return self.conversations_dir / f"{conversation_id}.json"
    
    def _extract_title_from_messages(self, messages: List[Dict]) -> str:
        """ä»æ¶ˆæ¯ä¸­æå–æ ‡é¢˜"""
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªç”¨æˆ·æ¶ˆæ¯ä½œä¸ºæ ‡é¢˜
        for msg in messages:
            if msg.get("role") == "user":
                content = msg.get("content", "").strip()
                if content:
                    # å–å‰50ä¸ªå­—ç¬¦ä½œä¸ºæ ‡é¢˜
                    title = content[:50]
                    if len(content) > 50:
                        title += "..."
                    return title
        return "æ–°å¯¹è¯"
    
    def _count_tools_in_messages(self, messages: List[Dict]) -> int:
        """ç»Ÿè®¡æ¶ˆæ¯ä¸­çš„å·¥å…·è°ƒç”¨æ•°é‡"""
        tool_count = 0
        for msg in messages:
            if msg.get("role") == "assistant" and "tool_calls" in msg:
                tool_calls = msg.get("tool_calls", [])
                tool_count += len(tool_calls) if isinstance(tool_calls, list) else 0
            elif msg.get("role") == "tool":
                tool_count += 1
        return tool_count
    
    def _initialize_token_statistics(self) -> Dict:
        """åˆå§‹åŒ–Tokenç»Ÿè®¡ç»“æ„"""
        return {
            "total_input_tokens": 0,
            "total_output_tokens": 0,
            "updated_at": datetime.now().isoformat()
        }
    
    def _validate_token_statistics(self, data: Dict) -> Dict:
        """éªŒè¯å¹¶ä¿®å¤Tokenç»Ÿè®¡æ•°æ®"""
        token_stats = data.get("token_statistics", {})
        
        # ç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨
        if "total_input_tokens" not in token_stats:
            token_stats["total_input_tokens"] = 0
        if "total_output_tokens" not in token_stats:
            token_stats["total_output_tokens"] = 0
        if "updated_at" not in token_stats:
            token_stats["updated_at"] = datetime.now().isoformat()
        
        # ç¡®ä¿æ•°å€¼ç±»å‹æ­£ç¡®
        try:
            token_stats["total_input_tokens"] = int(token_stats["total_input_tokens"])
            token_stats["total_output_tokens"] = int(token_stats["total_output_tokens"])
        except (ValueError, TypeError):
            print("âš ï¸ Tokenç»Ÿè®¡æ•°æ®æŸåï¼Œé‡ç½®ä¸º0")
            token_stats["total_input_tokens"] = 0
            token_stats["total_output_tokens"] = 0
        
        data["token_statistics"] = token_stats
        return data
    
    def create_conversation(
        self, 
        project_path: str, 
        thinking_mode: bool = False,
        initial_messages: List[Dict] = None
    ) -> str:
        """
        åˆ›å»ºæ–°å¯¹è¯
        
        Args:
            project_path: é¡¹ç›®è·¯å¾„
            thinking_mode: æ€è€ƒæ¨¡å¼
            initial_messages: åˆå§‹æ¶ˆæ¯åˆ—è¡¨
        
        Returns:
            conversation_id: å¯¹è¯ID
        """
        conversation_id = self._generate_conversation_id()
        messages = initial_messages or []
        
        # åˆ›å»ºå¯¹è¯æ•°æ®
        conversation_data = {
            "id": conversation_id,
            "title": self._extract_title_from_messages(messages),
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat(),
            "messages": messages,
            "metadata": {
                "project_path": project_path,
                "thinking_mode": thinking_mode,
                "total_messages": len(messages),
                "total_tools": self._count_tools_in_messages(messages),
                "status": "active"
            },
            "token_statistics": self._initialize_token_statistics()  # æ–°å¢
        }
        
        # ä¿å­˜å¯¹è¯æ–‡ä»¶
        self._save_conversation_file(conversation_id, conversation_data)
        
        # æ›´æ–°ç´¢å¼•
        self._update_index(conversation_id, conversation_data)
        
        self.current_conversation_id = conversation_id
        print(f"ğŸ“ åˆ›å»ºæ–°å¯¹è¯: {conversation_id} - {conversation_data['title']}")
        
        return conversation_id
    
    def _save_conversation_file(self, conversation_id: str, data: Dict):
        """ä¿å­˜å¯¹è¯æ–‡ä»¶"""
        try:
            # ç¡®ä¿Tokenç»Ÿè®¡æ•°æ®æœ‰æ•ˆ
            data = self._validate_token_statistics(data)
            
            file_path = self._get_conversation_file_path(conversation_id)
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âŒ˜ ä¿å­˜å¯¹è¯æ–‡ä»¶å¤±è´¥ {conversation_id}: {e}")
    
    def _update_index(self, conversation_id: str, conversation_data: Dict):
        """æ›´æ–°å¯¹è¯ç´¢å¼•"""
        try:
            index = self._load_index()
            
            # åˆ›å»ºå…ƒæ•°æ®
            metadata = ConversationMetadata(
                id=conversation_id,
                title=conversation_data["title"],
                created_at=conversation_data["created_at"],
                updated_at=conversation_data["updated_at"],
                project_path=conversation_data["metadata"]["project_path"],
                thinking_mode=conversation_data["metadata"]["thinking_mode"],
                total_messages=conversation_data["metadata"]["total_messages"],
                total_tools=conversation_data["metadata"]["total_tools"],
                status=conversation_data["metadata"].get("status", "active")
            )
            
            # æ·»åŠ åˆ°ç´¢å¼•
            index[conversation_id] = {
                "title": metadata.title,
                "created_at": metadata.created_at,
                "updated_at": metadata.updated_at,
                "project_path": metadata.project_path,
                "thinking_mode": metadata.thinking_mode,
                "total_messages": metadata.total_messages,
                "total_tools": metadata.total_tools,
                "status": metadata.status
            }
            
            self._save_index(index)
        except Exception as e:
            print(f"âŒ˜ æ›´æ–°å¯¹è¯ç´¢å¼•å¤±è´¥: {e}")
    
    def save_conversation(
        self, 
        conversation_id: str, 
        messages: List[Dict],
        project_path: str = None,
        thinking_mode: bool = None
    ) -> bool:
        """
        ä¿å­˜å¯¹è¯ï¼ˆæ›´æ–°ç°æœ‰å¯¹è¯ï¼‰
        
        Args:
            conversation_id: å¯¹è¯ID
            messages: æ¶ˆæ¯åˆ—è¡¨
            project_path: é¡¹ç›®è·¯å¾„
            thinking_mode: æ€è€ƒæ¨¡å¼
        
        Returns:
            bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        try:
            # åŠ è½½ç°æœ‰å¯¹è¯æ•°æ®
            existing_data = self.load_conversation(conversation_id)
            if not existing_data:
                print(f"âš ï¸ å¯¹è¯ {conversation_id} ä¸å­˜åœ¨ï¼Œæ— æ³•æ›´æ–°")
                return False
            
            # æ›´æ–°æ•°æ®
            existing_data["messages"] = messages
            existing_data["updated_at"] = datetime.now().isoformat()
            
            # æ›´æ–°æ ‡é¢˜ï¼ˆå¦‚æœæ¶ˆæ¯å‘ç”Ÿå˜åŒ–ï¼‰
            new_title = self._extract_title_from_messages(messages)
            if new_title != "æ–°å¯¹è¯":
                existing_data["title"] = new_title
            
            # æ›´æ–°å…ƒæ•°æ®
            if project_path is not None:
                existing_data["metadata"]["project_path"] = project_path
            if thinking_mode is not None:
                existing_data["metadata"]["thinking_mode"] = thinking_mode
            
            existing_data["metadata"]["total_messages"] = len(messages)
            existing_data["metadata"]["total_tools"] = self._count_tools_in_messages(messages)
            
            # ç¡®ä¿Tokenç»Ÿè®¡ç»“æ„å­˜åœ¨ï¼ˆå‘åå…¼å®¹ï¼‰
            if "token_statistics" not in existing_data:
                existing_data["token_statistics"] = self._initialize_token_statistics()
            else:
                existing_data["token_statistics"]["updated_at"] = datetime.now().isoformat()
            
            # ä¿å­˜æ–‡ä»¶
            self._save_conversation_file(conversation_id, existing_data)
            
            # æ›´æ–°ç´¢å¼•
            self._update_index(conversation_id, existing_data)
            
            return True
        except Exception as e:
            print(f"âŒ˜ ä¿å­˜å¯¹è¯å¤±è´¥ {conversation_id}: {e}")
            return False
    
    def load_conversation(self, conversation_id: str) -> Optional[Dict]:
        """
        åŠ è½½å¯¹è¯æ•°æ®
        
        Args:
            conversation_id: å¯¹è¯ID
        
        Returns:
            Dict: å¯¹è¯æ•°æ®ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å›None
        """
        try:
            file_path = self._get_conversation_file_path(conversation_id)
            if not file_path.exists():
                return None
            
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()
                if not content:
                    return None
                
                data = json.loads(content)
                
                # å‘åå…¼å®¹ï¼šç¡®ä¿Tokenç»Ÿè®¡ç»“æ„å­˜åœ¨
                if "token_statistics" not in data:
                    data["token_statistics"] = self._initialize_token_statistics()
                    # è‡ªåŠ¨ä¿å­˜ä¿®å¤åçš„æ•°æ®
                    self._save_conversation_file(conversation_id, data)
                    print(f"ğŸ”§ ä¸ºå¯¹è¯ {conversation_id} æ·»åŠ Tokenç»Ÿè®¡ç»“æ„")
                else:
                    # éªŒè¯ç°æœ‰Tokenç»Ÿè®¡æ•°æ®
                    data = self._validate_token_statistics(data)
                
                return data
        except (json.JSONDecodeError, Exception) as e:
            print(f"âŒ˜ åŠ è½½å¯¹è¯å¤±è´¥ {conversation_id}: {e}")
            return None
    
    def update_token_statistics(self, conversation_id: str, input_tokens: int, output_tokens: int) -> bool:
        """
        æ›´æ–°å¯¹è¯çš„Tokenç»Ÿè®¡
        
        Args:
            conversation_id: å¯¹è¯ID
            input_tokens: è¾“å…¥Tokenæ•°é‡
            output_tokens: è¾“å‡ºTokenæ•°é‡
        
        Returns:
            bool: æ›´æ–°æ˜¯å¦æˆåŠŸ
        """
        try:
            conversation_data = self.load_conversation(conversation_id)
            if not conversation_data:
                print(f"âš ï¸ æ— æ³•æ‰¾åˆ°å¯¹è¯ {conversation_id}ï¼Œè·³è¿‡Tokenç»Ÿè®¡")
                return False
            
            # ç¡®ä¿Tokenç»Ÿè®¡ç»“æ„å­˜åœ¨
            if "token_statistics" not in conversation_data:
                conversation_data["token_statistics"] = self._initialize_token_statistics()
            
            # æ›´æ–°ç»Ÿè®¡æ•°æ®
            token_stats = conversation_data["token_statistics"]
            token_stats["total_input_tokens"] = token_stats.get("total_input_tokens", 0) + input_tokens
            token_stats["total_output_tokens"] = token_stats.get("total_output_tokens", 0) + output_tokens
            token_stats["updated_at"] = datetime.now().isoformat()
            
            # ä¿å­˜æ›´æ–°
            self._save_conversation_file(conversation_id, conversation_data)
            
            print(f"ğŸ“Š Tokenç»Ÿè®¡å·²æ›´æ–°: +{input_tokens}è¾“å…¥, +{output_tokens}è¾“å‡º "
                  f"(æ€»è®¡: {token_stats['total_input_tokens']}è¾“å…¥, {token_stats['total_output_tokens']}è¾“å‡º)")
            
            return True
        except Exception as e:
            print(f"âŒ˜ æ›´æ–°Tokenç»Ÿè®¡å¤±è´¥ {conversation_id}: {e}")
            return False
    
    def get_token_statistics(self, conversation_id: str) -> Optional[Dict]:
        """
        è·å–å¯¹è¯çš„Tokenç»Ÿè®¡
        
        Args:
            conversation_id: å¯¹è¯ID
        
        Returns:
            Dict: Tokenç»Ÿè®¡æ•°æ®
        """
        try:
            conversation_data = self.load_conversation(conversation_id)
            if not conversation_data:
                return None
            
            token_stats = conversation_data.get("token_statistics", {})
            
            # ç¡®ä¿åŸºæœ¬å­—æ®µå­˜åœ¨
            result = {
                "total_input_tokens": token_stats.get("total_input_tokens", 0),
                "total_output_tokens": token_stats.get("total_output_tokens", 0),
                "total_tokens": token_stats.get("total_input_tokens", 0) + token_stats.get("total_output_tokens", 0),
                "updated_at": token_stats.get("updated_at"),
                "conversation_id": conversation_id
            }
            
            return result
        except Exception as e:
            print(f"âŒ˜ è·å–Tokenç»Ÿè®¡å¤±è´¥ {conversation_id}: {e}")
            return None
    
    def get_conversation_list(self, limit: int = 50, offset: int = 0) -> Dict:
        """
        è·å–å¯¹è¯åˆ—è¡¨
        
        Args:
            limit: é™åˆ¶æ•°é‡
            offset: åç§»é‡
        
        Returns:
            Dict: åŒ…å«å¯¹è¯åˆ—è¡¨å’Œç»Ÿè®¡ä¿¡æ¯
        """
        try:
            index = self._load_index()
            
            # æŒ‰æ›´æ–°æ—¶é—´å€’åºæ’åˆ—
            sorted_conversations = sorted(
                index.items(),
                key=lambda x: x[1].get("updated_at", ""),
                reverse=True
            )
            
            # åˆ†é¡µ
            total = len(sorted_conversations)
            conversations = sorted_conversations[offset:offset+limit]
            
            # æ ¼å¼åŒ–ç»“æœ
            result = []
            for conv_id, metadata in conversations:
                result.append({
                    "id": conv_id,
                    "title": metadata.get("title", "æœªå‘½åå¯¹è¯"),
                    "created_at": metadata.get("created_at"),
                    "updated_at": metadata.get("updated_at"),
                    "project_path": metadata.get("project_path"),
                    "thinking_mode": metadata.get("thinking_mode", False),
                    "total_messages": metadata.get("total_messages", 0),
                    "total_tools": metadata.get("total_tools", 0),
                    "status": metadata.get("status", "active")
                })
            
            return {
                "conversations": result,
                "total": total,
                "limit": limit,
                "offset": offset,
                "has_more": offset + limit < total
            }
        except Exception as e:
            print(f"âŒ˜ è·å–å¯¹è¯åˆ—è¡¨å¤±è´¥: {e}")
            return {
                "conversations": [],
                "total": 0,
                "limit": limit,
                "offset": offset,
                "has_more": False
            }
    
    def delete_conversation(self, conversation_id: str) -> bool:
        """
        åˆ é™¤å¯¹è¯
        
        Args:
            conversation_id: å¯¹è¯ID
        
        Returns:
            bool: åˆ é™¤æ˜¯å¦æˆåŠŸ
        """
        try:
            # åˆ é™¤å¯¹è¯æ–‡ä»¶
            file_path = self._get_conversation_file_path(conversation_id)
            if file_path.exists():
                file_path.unlink()
            
            # ä»ç´¢å¼•ä¸­åˆ é™¤
            index = self._load_index()
            if conversation_id in index:
                del index[conversation_id]
                self._save_index(index)
            
            # å¦‚æœåˆ é™¤çš„æ˜¯å½“å‰å¯¹è¯ï¼Œæ¸…é™¤å½“å‰å¯¹è¯ID
            if self.current_conversation_id == conversation_id:
                self.current_conversation_id = None
            
            print(f"ğŸ—‘ï¸ å·²åˆ é™¤å¯¹è¯: {conversation_id}")
            return True
        except Exception as e:
            print(f"âŒ˜ åˆ é™¤å¯¹è¯å¤±è´¥ {conversation_id}: {e}")
            return False
    
    def archive_conversation(self, conversation_id: str) -> bool:
        """
        å½’æ¡£å¯¹è¯ï¼ˆæ ‡è®°ä¸ºå·²å½’æ¡£ï¼Œä¸åˆ é™¤ï¼‰
        
        Args:
            conversation_id: å¯¹è¯ID
        
        Returns:
            bool: å½’æ¡£æ˜¯å¦æˆåŠŸ
        """
        try:
            # æ›´æ–°å¯¹è¯çŠ¶æ€
            conversation_data = self.load_conversation(conversation_id)
            if not conversation_data:
                return False
            
            conversation_data["metadata"]["status"] = "archived"
            conversation_data["updated_at"] = datetime.now().isoformat()
            
            # ä¿å­˜æ›´æ–°
            self._save_conversation_file(conversation_id, conversation_data)
            self._update_index(conversation_id, conversation_data)
            
            print(f"ğŸ“¦ å·²å½’æ¡£å¯¹è¯: {conversation_id}")
            return True
        except Exception as e:
            print(f"âŒ˜ å½’æ¡£å¯¹è¯å¤±è´¥ {conversation_id}: {e}")
            return False
    
    def search_conversations(self, query: str, limit: int = 20) -> List[Dict]:
        """
        æœç´¢å¯¹è¯
        
        Args:
            query: æœç´¢å…³é”®è¯
            limit: é™åˆ¶æ•°é‡
        
        Returns:
            List[Dict]: åŒ¹é…çš„å¯¹è¯åˆ—è¡¨
        """
        try:
            index = self._load_index()
            results = []
            
            query_lower = query.lower()
            
            for conv_id, metadata in index.items():
                # æœç´¢æ ‡é¢˜
                title = metadata.get("title", "").lower()
                if query_lower in title:
                    score = 100  # æ ‡é¢˜åŒ¹é…æƒé‡æœ€é«˜
                    results.append((score, {
                        "id": conv_id,
                        "title": metadata.get("title"),
                        "created_at": metadata.get("created_at"),
                        "updated_at": metadata.get("updated_at"),
                        "project_path": metadata.get("project_path"),
                        "match_type": "title"
                    }))
                    continue
                
                # æœç´¢é¡¹ç›®è·¯å¾„
                project_path = metadata.get("project_path", "").lower()
                if query_lower in project_path:
                    results.append((50, {
                        "id": conv_id,
                        "title": metadata.get("title"),
                        "created_at": metadata.get("created_at"),
                        "updated_at": metadata.get("updated_at"),
                        "project_path": metadata.get("project_path"),
                        "match_type": "project_path"
                    }))
            
            # æŒ‰åˆ†æ•°æ’åº
            results.sort(key=lambda x: x[0], reverse=True)
            
            # è¿”å›å‰Nä¸ªç»“æœ
            return [result[1] for result in results[:limit]]
        except Exception as e:
            print(f"âŒ˜ æœç´¢å¯¹è¯å¤±è´¥: {e}")
            return []
    
    def cleanup_old_conversations(self, days: int = 30) -> int:
        """
        æ¸…ç†æ—§å¯¹è¯ï¼ˆå¯é€‰åŠŸèƒ½ï¼‰
        
        Args:
            days: ä¿ç•™å¤©æ•°
        
        Returns:
            int: æ¸…ç†çš„å¯¹è¯æ•°é‡
        """
        try:
            from datetime import datetime, timedelta
            cutoff_date = datetime.now() - timedelta(days=days)
            cutoff_iso = cutoff_date.isoformat()
            
            index = self._load_index()
            to_delete = []
            
            for conv_id, metadata in index.items():
                updated_at = metadata.get("updated_at", "")
                if updated_at < cutoff_iso and metadata.get("status") != "archived":
                    to_delete.append(conv_id)
            
            deleted_count = 0
            for conv_id in to_delete:
                if self.delete_conversation(conv_id):
                    deleted_count += 1
            
            if deleted_count > 0:
                print(f"ğŸ§¹ æ¸…ç†äº† {deleted_count} ä¸ªæ—§å¯¹è¯")
            
            return deleted_count
        except Exception as e:
            print(f"âŒ˜ æ¸…ç†æ—§å¯¹è¯å¤±è´¥: {e}")
            return 0
    
    def get_statistics(self) -> Dict:
        """
        è·å–å¯¹è¯ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            Dict: ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            index = self._load_index()
            
            total_conversations = len(index)
            total_messages = sum(meta.get("total_messages", 0) for meta in index.values())
            total_tools = sum(meta.get("total_tools", 0) for meta in index.values())
            
            # æŒ‰çŠ¶æ€åˆ†ç±»
            status_count = {}
            for metadata in index.values():
                status = metadata.get("status", "active")
                status_count[status] = status_count.get(status, 0) + 1
            
            # æŒ‰æ€è€ƒæ¨¡å¼åˆ†ç±»
            thinking_mode_count = {
                "thinking": sum(1 for meta in index.values() if meta.get("thinking_mode")),
                "fast": sum(1 for meta in index.values() if not meta.get("thinking_mode"))
            }
            
            # æ–°å¢ï¼šTokenç»Ÿè®¡æ±‡æ€»
            total_input_tokens = 0
            total_output_tokens = 0
            token_stats_count = 0
            
            for conv_id in index.keys():
                token_stats = self.get_token_statistics(conv_id)
                if token_stats:
                    total_input_tokens += token_stats.get("total_input_tokens", 0)
                    total_output_tokens += token_stats.get("total_output_tokens", 0)
                    token_stats_count += 1
            
            return {
                "total_conversations": total_conversations,
                "total_messages": total_messages,
                "total_tools": total_tools,
                "status_distribution": status_count,
                "thinking_mode_distribution": thinking_mode_count,
                "token_statistics": {
                    "total_input_tokens": total_input_tokens,
                    "total_output_tokens": total_output_tokens,
                    "total_tokens": total_input_tokens + total_output_tokens,
                    "conversations_with_stats": token_stats_count
                }
            }
        except Exception as e:
            print(f"âŒ˜ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {}
    
    def get_current_conversation_id(self) -> Optional[str]:
        """è·å–å½“å‰å¯¹è¯ID"""
        return self.current_conversation_id
    
    def set_current_conversation_id(self, conversation_id: str):
        """è®¾ç½®å½“å‰å¯¹è¯ID"""
        self.current_conversation_id = conversation_id
    
    def calculate_conversation_tokens(self, conversation_id: str, context_manager=None, focused_files=None, terminal_content="") -> dict:
        """è®¡ç®—å¯¹è¯çš„çœŸå®API tokenæ¶ˆè€—"""
        try:
            if not context_manager:
                return {"total_tokens": 0}
            
            conversation_data = self.load_conversation(conversation_id)
            if not conversation_data:
                return {"total_tokens": 0}
            
            # æ„å»ºcontextå’Œmessages...
            context = context_manager.build_main_context(memory_content="")
            messages = context_manager.build_messages(context, "")
            
            # è®¡ç®—æ¶ˆæ¯token
            message_tokens = context_manager.calculate_input_tokens(messages, [])
            
            # ç¡¬ç¼–ç æ·»åŠ å·¥å…·å®šä¹‰token
            tools_tokens = 2400  # åŸºäºä½ çš„æ—¥å¿—
            
            total_tokens = message_tokens + tools_tokens
            
            return {"total_tokens": total_tokens}
            
        except Exception as e:
            print(f"è®¡ç®—tokenå¤±è´¥: {e}")
            return {"total_tokens": 0}
    def _get_tools_definition(self, context_manager):
        """è·å–å·¥å…·å®šä¹‰"""
        try:
            # éœ€è¦æ‰¾åˆ°å·¥å…·å®šä¹‰çš„æ¥æºï¼Œé€šå¸¸åœ¨ main_terminal ä¸­
            # ä½ éœ€è¦æ‰¾åˆ° main_terminal çš„å¼•ç”¨æˆ–è€… define_tools æ–¹æ³•
            
            # æ–¹æ³•1: å¦‚æœ context_manager æœ‰ main_terminal å¼•ç”¨
            if hasattr(context_manager, 'main_terminal') and context_manager.main_terminal:
                return context_manager.main_terminal.define_tools()
            
            # æ–¹æ³•2: å¦‚æœæœ‰å…¶ä»–æ–¹å¼è·å–å·¥å…·å®šä¹‰
            # ä½ éœ€è¦å»æ‰¾ä¸€ä¸‹åœ¨å“ªé‡Œè°ƒç”¨äº† calculate_input_tokensï¼Œçœ‹çœ‹ tools å‚æ•°æ˜¯æ€ä¹ˆä¼ çš„
            
            return []
        except Exception as e:
            print(f"è·å–å·¥å…·å®šä¹‰å¤±è´¥: {e}")
            return []